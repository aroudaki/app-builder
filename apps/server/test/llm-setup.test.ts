/**
 * LLM Configuration Test
 * 
 * This test validates the LLM setup and configuration for Phase 2 Task 2.1
 */

import {
    createLLM,
    createGPT41LLM,
    createO3LLM,
    createLLMForAgent,
    testLLMConnection,
    getAvailableModelTypes,
    isModelAvailable,
    type ModelType
} from '../src/langgraph/llm.js';

async function testLLMConfiguration() {
    console.log("ðŸ§ª Testing LLM Configuration - Phase 2 Task 2.1");
    console.log("=".repeat(50));

    // Test 1: Check available model types
    console.log("\n1ï¸âƒ£ Testing Available Model Types");
    const modelTypes = getAvailableModelTypes();
    console.log("   Available models:", modelTypes);

    // Test 2: Check environment configuration
    console.log("\n2ï¸âƒ£ Testing Environment Configuration");
    modelTypes.forEach(modelType => {
        const isAvailable = isModelAvailable(modelType);
        console.log(`   ${modelType}: ${isAvailable ? 'âœ… Configured' : 'âŒ Missing config'}`);
    });

    // Test 3: Create LLM instances
    console.log("\n3ï¸âƒ£ Testing LLM Instance Creation");

    try {
        const gpt41LLM = createGPT41LLM({ temperature: 0.3, streaming: false });
        console.log("   âœ… GPT-4.1 LLM instance created successfully");
        console.log(`   - Temperature: ${gpt41LLM.temperature}`);
        console.log(`   - Streaming: ${gpt41LLM.streaming}`);
    } catch (error) {
        console.log("   âŒ GPT-4.1 LLM creation failed:", error.message);
    }

    try {
        const o3LLM = createO3LLM({ temperature: 0.1, streaming: false });
        console.log("   âœ… O3 LLM instance created successfully");
        console.log(`   - Temperature: ${o3LLM.temperature}`);
        console.log(`   - Streaming: ${o3LLM.streaming}`);
    } catch (error) {
        console.log("   âŒ O3 LLM creation failed:", error.message);
    }

    // Test 4: Agent-specific LLM creation
    console.log("\n4ï¸âƒ£ Testing Agent-Specific LLM Creation");
    const agentTypes = ['clarification', 'requirements', 'wireframe', 'coding', 'modification'] as const;

    agentTypes.forEach(agentType => {
        try {
            const agentLLM = createLLMForAgent(agentType, { streaming: false });
            const expectedModel = agentType === 'coding' ? 'O3' : 'GPT-4.1';
            console.log(`   âœ… ${agentType} agent LLM created (${expectedModel})`);
        } catch (error) {
            console.log(`   âŒ ${agentType} agent LLM creation failed:`, error.message);
        }
    });

    // Test 5: Default LLM creation
    console.log("\n5ï¸âƒ£ Testing Default LLM Creation");
    try {
        const defaultLLM = createLLM();
        console.log("   âœ… Default LLM (GPT-4.1) created successfully");

        const customLLM = createLLM('o3', { temperature: 0.05, maxTokens: 2000 });
        console.log("   âœ… Custom O3 LLM created successfully");
        console.log(`   - Temperature: ${customLLM.temperature}`);
        console.log(`   - Max Tokens: ${customLLM.maxTokens}`);
    } catch (error) {
        console.log("   âŒ Default LLM creation failed:", error.message);
    }

    // Test 6: Connection test (if environment is configured)
    console.log("\n6ï¸âƒ£ Testing LLM Connectivity");

    if (isModelAvailable('gpt-4.1')) {
        try {
            console.log("   ðŸ”„ Testing GPT-4.1 connection...");
            await testLLMConnection('gpt-4.1');
        } catch (error) {
            console.log("   âš ï¸ GPT-4.1 connection test failed:", error.message);
            console.log("   Note: This is expected if Azure OpenAI credentials are not configured");
        }
    } else {
        console.log("   â­ï¸ Skipping GPT-4.1 connection test (not configured)");
    }

    if (isModelAvailable('o3')) {
        try {
            console.log("   ðŸ”„ Testing O3 connection...");
            await testLLMConnection('o3');
        } catch (error) {
            console.log("   âš ï¸ O3 connection test failed:", error.message);
            console.log("   Note: This is expected if Azure OpenAI credentials are not configured");
        }
    } else {
        console.log("   â­ï¸ Skipping O3 connection test (not configured)");
    }

    // Test Summary
    console.log("\n" + "=".repeat(50));
    console.log("ðŸ“Š LLM Configuration Test Summary");
    console.log("=".repeat(50));

    const gpt41Available = isModelAvailable('gpt-4.1');
    const o3Available = isModelAvailable('o3');

    console.log(`âœ… LLM Module: Loaded successfully`);
    console.log(`${gpt41Available ? 'âœ…' : 'âš ï¸'} GPT-4.1: ${gpt41Available ? 'Ready' : 'Needs configuration'}`);
    console.log(`${o3Available ? 'âœ…' : 'âš ï¸'} O3: ${o3Available ? 'Ready' : 'Needs configuration'}`);
    console.log(`âœ… Factory Functions: All working`);
    console.log(`âœ… Agent Integration: Ready`);

    if (gpt41Available || o3Available) {
        console.log(`\nðŸŽ‰ Phase 2 Task 2.1 (OpenAI/Azure OpenAI Setup): COMPLETE!`);
        console.log(`Ready for Phase 2 Task 2.2: Prompt Templates`);
    } else {
        console.log(`\nâš ï¸ Phase 2 Task 2.1: Module ready, but needs Azure OpenAI credentials`);
        console.log(`Set environment variables in .env file to complete setup`);
    }
}

// Run the test
testLLMConfiguration().catch(error => {
    console.error("Test failed:", error);
    process.exit(1);
});
